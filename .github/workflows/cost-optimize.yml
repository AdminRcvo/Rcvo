name: Cost Optimize - Daily Check

on:
  schedule:
    - cron: '0 7 * * *' # every day at 07:00 UTC
  workflow_dispatch:
  push:
    branches: ["main"]

permissions:
  contents: read

jobs:
  cost-analyze:
    name: Cost analysis (safe)
    runs-on: ubuntu-latest

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      S3_BUCKET: ${{ secrets.S3_BUCKET }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir boto3 pyyaml

      - name: Validate workflow YAML files (quick parse)
        run: |
          python - <<'PY'
          import sys, yaml, glob
          ok = True
          for f in glob.glob('.github/workflows/*.yml') + glob.glob('.github/workflows/*.yaml'):
              try:
                  yaml.safe_load(open(f, 'r'))
              except Exception as e:
                  print(f"YAML PARSE ERROR in {f}: {e}", file=sys.stderr)
                  ok = False
          if not ok:
              sys.exit(2)
          print("All workflow YAML parse OK")
          PY

      - name: Gather AWS identity (read-only)
        run: |
          set -euo pipefail
          aws --version || true
          aws sts get-caller-identity --output json || true

      - name: "Optional: Run cost estimation script if present"
        run: |
          set -euo pipefail
          if [ -x "./scripts/cost-optimize.sh" ]; then
            echo "Found ./scripts/cost-optimize.sh — executing (safe mode)"
            ./scripts/cost-optimize.sh || echo "script returned non-zero (allowed)"
          else
            echo "No ./scripts/cost-optimize.sh found — skipping"
          fi

      - name: Quick S3 storage-class check (non-destructive)
        if: ${{ secrets.S3_BUCKET != '' }}
        run: |
          set -euo pipefail
          KEY_BASE="cost-optimize-check-$(date -u +%Y%m%dT%H%M%SZ)-${GITHUB_RUN_ID}"
          TEMP_KEY="tmp/${KEY_BASE}.txt"
          ARCHIVE_KEY="archive/${KEY_BASE}.txt"
          echo "ping" > "/tmp/${KEY_BASE}.txt"

          aws s3 cp "/tmp/${KEY_BASE}.txt" "s3://${{ secrets.S3_BUCKET }}/${TEMP_KEY}" --region "${{ secrets.AWS_REGION }}" --only-show-errors
          aws s3 cp "s3://${{ secrets.S3_BUCKET }}/${TEMP_KEY}" "s3://${{ secrets.S3_BUCKET }}/${ARCHIVE_KEY}" --region "${{ secrets.AWS_REGION }}" --storage-class STANDARD --only-show-errors

          SC=$(aws s3api head-object --bucket "${{ secrets.S3_BUCKET }}" --key "${ARCHIVE_KEY}" --query 'StorageClass' --output text --region "${{ secrets.AWS_REGION }}" || echo "")
          echo "Raw StorageClass read: '${SC}'"
          if [ "${SC}" = "STANDARD" ] || [ -z "${SC}" ] || [ "${SC}" = "None" ] || [ "${SC}" = "null" ]; then
            echo "StorageClass OK (interpreted as STANDARD)."
          else
            echo "WARNING: StorageClass is not STANDARD: ${SC}"
          fi

          aws s3 rm "s3://${{ secrets.S3_BUCKET }}/${TEMP_KEY}" --region "${{ secrets.AWS_REGION }}" >/dev/null 2>&1 || true
          aws s3 rm "s3://${{ secrets.S3_BUCKET }}/${ARCHIVE_KEY}" --region "${{ secrets.AWS_REGION }}" >/dev/null 2>&1 || true
          rm -f "/tmp/${KEY_BASE}.txt" || true

      - name: Report summary
        run: |
          echo "Cost Optimize job finished. No destructive actions performed by default."
